{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "6\n",
      "The entered term is :  author\n",
      "Listing for term :  author\n",
      "TermID :  899\n",
      "Number of documents containing term :  4\n",
      "Term frequency in corpus :  10\n",
      "Time:  -28113.8046601\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import html2text\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "print(\"start\")\n",
    "porter = PorterStemmer()\n",
    "docid=1\n",
    "termid=1\n",
    "##-------------------------------tokenization of stoplist\n",
    "f=open(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\stoplist.txt')\n",
    "stoplist=f.read()\n",
    "h = html2text.HTML2Text()\n",
    "parse=h.handle(stoplist)\n",
    "stoplist=parse.split(\" \")\n",
    "z=[]\n",
    "y=[]\n",
    "##-------------------------------reading\n",
    "entries = os.listdir(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\corpus')\n",
    "print(len(entries)) ## no of files in the given directory\n",
    "for filename in os.listdir(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\corpus'):\n",
    "        with open(os.path.join(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\corpus', filename), 'r', encoding='utf8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "##-------------------------------tokenization\n",
    "            h = html2text.HTML2Text()\n",
    "            h.ignore_links = True\n",
    "            parse=h.handle(content)\n",
    "            x=parse.split(\" \")\n",
    "            z.append(x)  \n",
    "for i in range(len(z)):\n",
    "    for j in (range(len(z[i]))):\n",
    "        y.append(z[i][j])\n",
    "\n",
    "##-------------------------------lower tokenization\n",
    "for i in range(len (y)):\n",
    "    y[i]=y[i].lower()\n",
    "##-------------------------------stopwords portion\n",
    "count=0\n",
    "for i in range (len(y)):\n",
    "    y[i]=porter.stem(y[i])                                                #Stemming done here\n",
    "    y[i]= re.sub('['+string.punctuation+']', '', y[i])                    #Punctuation Removed here\n",
    "    y[i]=y[i].replace('\\n', \"\")\n",
    "    for j in range (len(stoplist)):\n",
    "        if y[i]==stoplist[j]:\n",
    "            str=stoplist[j]\n",
    "            y[i]=y[i].replace(stoplist[j],\"\")\n",
    "            count+=1\n",
    "y = list(filter(None, y))\n",
    "##--------------------------------remove duplicates\n",
    "x = set(y)\n",
    "x = list(x)        \n",
    "# print(x)\n",
    "open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\docid.txt\",\"w\")\n",
    "open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\termid.txt\",\"w\")\n",
    "for i in range (len(x)):\n",
    "##writing in the termids.txt            \n",
    "    with open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\termid.txt\", \"a\", encoding='utf8', errors='ignore') as myfile1:\n",
    "        myfile1.write('%d' % termid)\n",
    "        myfile1.write(\"\\t\")\n",
    "        myfile1.write(x[i].replace('\\n', \"\"))\n",
    "        myfile1.write(\"\\n\")\n",
    "        termid=termid+1\n",
    "##-------------------------------writing in the docids.txt\n",
    "for filename in os.listdir('C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\corpus'):\n",
    "    with open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\docid.txt\", \"a\", encoding='utf8', errors='ignore') as myfile:\n",
    "        myfile.write('%d' % docid)\n",
    "        myfile.write(\"\\t\")\n",
    "        myfile.write(filename)\n",
    "        myfile.write(\"\\n\")\n",
    "        docid=docid+1\n",
    "myfile.close()\n",
    "open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\term_index.txt\",\"w\")\n",
    "for i in range(termid-1):\n",
    "    docID=[]\n",
    "    pos=[]\n",
    "    for j in range(len(z)):\n",
    "        for k in (range(len(z[j]))):\n",
    "            if x[i] in re.sub('['+string.punctuation+']', '', z[j][k].lower().replace('\\n', \"\")):\n",
    "                docID.append(j)\n",
    "                pos.append(k)\n",
    "##-------------------------------writing in term_index.txt              \n",
    "    with open(\"C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\term_index.txt\", \"a\", encoding='utf8', errors='ignore') as myfile:\n",
    "        myfile.write('%d' % (i+1))\n",
    "        myfile.write(\" \")\n",
    "        myfile.write('%d' % len(docID))\n",
    "        myfile.write(\" \")\n",
    "        setdocID=set(docID)\n",
    "        myfile.write('%d' % len(setdocID))\n",
    "        myfile.write(\" \")\n",
    "        counter=0\n",
    "        for Ids in range(len(docID)):\n",
    "            counter=counter+1\n",
    "            if counter == 1:\n",
    "                myfile.write('%d' % (docID[Ids]+1))\n",
    "                myfile.write(\",\")\n",
    "                myfile.write('%d' % (pos[Ids]+1))\n",
    "                myfile.write(\" \")\n",
    "            elif docID[Ids]-docID[Ids-1] == 0:\n",
    "                myfile.write('%d' % (docID[Ids]-docID[Ids-1]))\n",
    "                myfile.write(\",\")\n",
    "                myfile.write('%d' % (pos[Ids]-pos[Ids-1]))\n",
    "                myfile.write(\" \")\n",
    "            else:\n",
    "                myfile.write('%d' % (docID[Ids]-docID[Ids-1]))\n",
    "                myfile.write(\",\")\n",
    "                myfile.write('%d' % (pos[Ids]+1))\n",
    "                myfile.write(\" \")\n",
    "        myfile.write(\"\\n\")\n",
    "a={}\n",
    "theFile=open(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\term_index.txt')\n",
    "mapping=theFile.read().split()\n",
    "# print(len(mapping))\n",
    "\n",
    "##--------------------------------hashmaps\n",
    "start=0\n",
    "end=len(mapping)\n",
    "for i in range (start,end):\n",
    "#     print(start)\n",
    "    if start==end:\n",
    "        break\n",
    "    key = mapping[start]\n",
    "    a.setdefault(key, [])\n",
    "    start=start+1\n",
    "    a[key].append(mapping[start])\n",
    "    total=int(mapping[start])\n",
    "    start=start+1\n",
    "    a[key].append(mapping[start])\n",
    "    for j in range (total):\n",
    "        start=start+1\n",
    "        a[key].append(mapping[start])\n",
    "    start=start+1\n",
    "##------------------------------- step 3\n",
    "#txt = input(\"Enter the term for its inverted index\\n\")\n",
    "txt='author'\n",
    "print(\"The entered term is : \", txt)\n",
    "temp=open(r'C:\\\\Users\\\\Sameed Khan Durrani\\\\Desktop\\\\LHR DS\\\\IR\\\\termid.txt')\n",
    "temp1=temp.read().split()\n",
    "if txt in temp1:\n",
    "    pos=temp1.index(txt)  \n",
    "    print(\"Listing for term : \", txt)\n",
    "    print(\"TermID : \" ,temp1[pos-1])\n",
    "    print(\"Number of documents containing term : \", a[temp1[pos-1]][1])\n",
    "    print(\"Term frequency in corpus : \" , a[temp1[pos-1]][0])\n",
    "else:\n",
    "    print(\"Sorry can't find such term\")\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
